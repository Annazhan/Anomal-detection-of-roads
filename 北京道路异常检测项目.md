# 北京道路异常检测项目



# 一、项目背景

## 1.1 简介

随着社会的进步和城市的发展，越来越多的家庭选择购买私家车，而城市的道路也在不断的建设并完善。虽然如此，城市道路中依然会出现拥堵，其原因可能是城市道路建设不完善或者是有突发状况发生，比如发生汽车追尾事件。城市的拥堵不仅会对人民的生活造成不变，也会造成巨大的经济损失。根据高德2018年中国主要城市交通分析报告 [^1]，北京是中国最拥堵的城市，人均一生（25岁到60岁）通勤堵车小时数为6260小时，而人均一生通勤小时数为12325。堵车时间占通勤时间的一半以上。为能及时的找到交通异常路段、及时处理交通异常事件，减少经济损失，我们需要建立一种能预测交通异常的模型。

在此次项目中，我们将交通异常定义为两类，一类为常规性异常，即：早晚高峰，大规模交通管制、以及极端天气引起的道路拥堵。第二类为突发性异常，即：由交通事故、大型活动等突发事件引起的道路拥堵。本次项目模型主要用于突发性异常的检测。

## 1.2 主要思路

本项目获取北京出租车9天的GPS数据，并认为出租车在北京道路上的车流量是北京市所有道路上所有车的车流量的特征。该数据经过数据清洗、非负矩阵分解以及聚类等大数据处理算法，建立“交通异常模型”，最后利用模型，针对当前交通流量数据，判定当前有哪些道路出现了交通异常。

我将通过以下几个方面来阐述模型，首先是原始数据的处理，包括数据清洗、道路匹配等；第二个方面是交通模式提取，主要运用非负矩阵分解；第三个方面是邻居道路发现，主要通过聚类的方法得到道路的分类；第四个部分为异常得分计算，通过第三个方面计算出的聚类，给当前道路计算邻居异常得分和历史异常得分；第五个方面为异常预测即结果。

## 1.3 实现工具

**收集数据：**考虑到GPS收集数据的成本可控，精度较高。且覆盖时间广，覆盖地域广。考虑到上述优势，本次项目通过GPS来采集数据。

**代码工具：**本次项目全部由python以及python的库来实现，由于python处理数据较快，python生态较为完善，能够简单的通过库的调用即能满足大多数数据处理，因此本次项目采用python，来实现交通异常模型并完成预测。



# 二、数据预处理

本项目获取的数据格式为一天为一个文件夹，每个文件夹下有文件，每一个文件是一台出租车再这一天内的所有数据。其中车辆GPS收集到的数据以及意义如下：

| 车辆ID         | 采样时间       | 经度           | 纬度           | 路段端点ID1                | 路段端点ID2                | 速度       | 方向             | 车辆状态           | ……   |
| -------------- | -------------- | -------------- | -------------- | -------------------------- | -------------------------- | ---------- | ---------------- | ------------------ | ---- |
| 采样车辆的编号 | YYYYMMDDhhmmss | 经度（浮点数） | 纬度（浮点数） | 路段端点1在OSM地图中的编号 | 路段端点2在OSM地图中的编号 | 单位为Km/h | 与正东方向的夹角 | 空车、载客、停运等 |      |

##  2.1 数据清洗

数据清洗是对收集的数据进行重新审查和校验的过程，目的在于删除重复信息，纠正存在错误的数据，并提供一致性的数据，一遍下一步的数据处理。如果缺少了数据清洗的过程，后面的模型建立和预测都无可避免出现错误。需要处理的数据主要类型为：残缺数据、错误造成、数据重复、数据的不一致等。

### 2.1.1 异常数据定义

基于本项目数据的实际意义和数据清洗的方法，我们定义了以下异常数据：

1. 不真实点：由于出租车不是经常在五环之外出现，因此我们将GPS的经纬度限定在北京五环之内，即经度在116.1988 ~ 116.5460之间，纬度在 39.7548 ~ 40.0246之间。
2. 重复时间点：某一次一GPS上传可能出现了重复上传，即所有的数据相同。
3. 高速点：由于北京5环内限速80Km/h，考虑到采样是数据的不准确性和可能存在的轻微超速，我们将超过90km/h的点定义为异常点。
4. 距离过长点：如果相邻两个点的距离超过2Km，我们认为是不真实点，由于GPS采样的时间差是45s左右，如果存在轻微超速，假设出租车行驶的速度为90Km/h，那么45s后，出租车行驶的距离为1.125Km。每两次采样点之间的距离最多为1.125Km。
5. 时间过长点：如果两个时间戳之间的差超过10min，则认为GPS出现故障。
6. 停驻点/等客点：如果在30min内，移动的距离小于50m，则认为出租车已经停下。本此项目只对正在道路上移动的出租车感兴趣，停驻点可以去掉。

### 2.1.2 异常数据处理方法

在上面我们已经定义了数据的异常情况，针对上述异常，我们采用以下不同方式处理：

1. 不真实点：直接将出现在范围外的数据删除。
2. 重复点：如果出现两个以上的点所有数据相同，则删除重复点只留下一行数据。
3. 高速点：如果出现速度超过90Km/h的数据，则将这一行数据全部删除
4. 距离过长点：通过两点之间的经纬度得出两点之间的距离，如果超出2Km，则将整个关于该ID的车辆数据全部删除，认为该车载GPS损坏。
5. 时间过长点：如果两个时间戳之间的差超过10min，也同样认为GPS损坏，将关于该ID的车辆数据全部删除。
6. 停驻点/等客点：如果出租车在30min内，移动的距离小于50m，则认为出租车停下。对于所有停下行数据，全部删除。

### 2.1.3  实际处理

在实际处理的过程中，各个异常之间的处理有先后顺序，比如应该先处理时间过长点，再处理停驻点/等客点。因为，如果将该数据倒过来，可能会出现误删的情况。我们按照距离过长、时间过长、停驻点/等客点、重复点、高速点和不真实点的顺序对数据进行删除操作。

我们在实际操作的过程中发现，数据没有严格的按照时间顺序排列，**存在倒序现象**。因此，对于这部分时间倒序，我们也是采取直接删除这一行数据的处理办法。

## 2.2 道路匹配

由于我们获得的数据格式是不同时间GPS的经纬度数据，但是考虑到GPS定位可能出现偏差，最终的定位可能没有出现在道路上。而且为了将GPS数据定位在某条道路上，通过道路匹配，将该车算入某个时间点和某条道路其中之一的车流量中。考虑到采样时间间隔为45s左右，属于低采样率的GPS轨迹，我们应当采用IVMM[^2]方法对车辆的路径进行道路匹配。

### 2.2.1 道路匹配的结果

对每一辆车的每一天的数据进行道路匹配，将车辆的GPS数据定位到某一条道路上。最后根据道路的信息和时段对车辆个数进行统计（每一天按照15分钟的时段划分）。同样按照不同的日期建立文件，最后得出的新的表格：

| 路段端点ID1                | 路段端点ID2                | 编号 | 0:00-0:15      | 0:15-0:30      | ……   |
| -------------------------- | -------------------------- | ---- | -------------- | -------------- | ---- |
| 根据OSM地图得到的路段端点1 | 根据OSM地图得到的路段端点1 | 行号 | 该时段的车流量 | 该时段的车流量 |      |

15分钟一个时间段，将一天划分为96段。

## 2.3 数据清洗结果

下面是初始数据经过上述处理得到的结果：

![](1.jpg)

### 2.3.1 存在问题

**1. 清洗时间过长点或者距离过长点**

导致时间过长和距离过长异常的因素可能是人为因素，比如司机在某一时段关闭了GPS，如果对这种数据采取全部清除，则可能造成删除数据过多的后果。对于这部分的处理，可以采取如果时间间隔大于5min，则重新将后一个点作为轨迹的开头，重新开始路段匹配。

**2. 道路匹配结果不准确**

最终的匹配结果存在噪声，可能有些道路匹配结果不准确。

# 三、交通模式提取

经过上述数据清洗，得到的数据有9个文件，每个文件有127049行数据，每行数据有96个时间段。数据的维数过大会对之后的模型造成过拟合的影响，从而使模型的预测结果较差。因此，需要对数据进行特征提取，以减小矩阵维数。

## 3.1 提取方法

本次项目决定采用非负矩阵分解，将大矩阵分为基矩阵和系数矩阵。

### 3.1.1 非负矩阵分解（NMF）原理

#### 3.1.1.1 符号表示

| 符号 |            含义             |
| :--: | :-------------------------: |
| $V$  |         带分解矩阵          |
| $W$  |       系数矩阵/基矩阵       |
| $H$  |       基矩阵/系数矩阵       |
| $r$  | 分解之后认为矩阵有$r$个特征 |
| $n$  |        原矩阵的行数         |
| $m$  |        原矩阵的列数         |

其中
$$
V = {[v_{ij}]}_{nm}, v_{ij} \geq 0
$$
$$
W = {[w_{ij}]}_{nm}, w_{ij} \geq 0
$$

$$
H = {[h_{ij}]}_{nm}, h_{ij} \geq 0
$$

且$r$需满足以下条件，才能说明对矩阵进行了降维和降噪处理，即$V \approx WH$：
$$
r \leq rank^+(V)
$$

#### 3.1.1.2 基本步骤

基本步骤如下图所示：

![](3.jpg)

其中$W_{nr},H_{rm}$为随机选取，但其中所有的项都在$[0,1]$之间能最快获得所需的分解因子[^3]，且$r$的选取要小于被分解矩阵的秩。

在欧氏距离下，随机初始化，然后不断迭代，迭代$W$和$H$的方法如下：
$$
H_{ij} \leftarrow H_{ij}\frac{{(W^TV)}_{ij}}{{(W^TWH)}_{ij}}
$$

$$
W_{ij} \leftarrow W_{ij}\frac{{(VH^T)}_{ij}}{{(WHH^T)}_{ij}},其中H为最新的H
$$



### 3.1.2 非负矩阵分解优势

**1. 对于稀疏矩阵**

在科学研究中，矩阵是最常被用到的一种处理大数据的数据表达方式。然而，数据直接对应的矩阵信息分布往往不均匀且维度极大，计算效率低，因此需要矩阵分解。在本次项目中，得到的车流量矩阵非常稀疏，零流量的占比较大，因此用矩阵分解可以解决该问题。

**2. 能够解释分解后的意义**

传统的矩阵分解不能保证数据的非负性，在数学上看，分解结果中存在负值是正常的，但负元素在实际问题中往往没有意义。而用NMF方法，可以保证分解之后的两个矩阵都是非负，如果分离到位，则可用特则和系数来对分解结果进行解释。

**3. 减小误差**

在NMF方法中，分解结果和原矩阵没有必要相等，即$V \approx HM$。通过这个式子可以看出，在矩阵分解之后，会消除一部分噪声，即$V-HM$的部分。

## 3.2 具体实现

### 3.2.1 自己代码实现

在自己实现的过程中，可以调用numpy库，来对矩阵进行计算。在自己实现的过程中有以下几点需要注意：

1. 除零处理：在进行更新计算的时候，可能存在除零的现象，因此，一种方法是可以设置如果除零，则结果等于零；另一种方法是，在结果后面加上大于零的极小值，如$10^{-8}$等
2. 循环结束的判断：在上面的流程图中，结束条件为$V \approx WH$，而在现实实现的过程中，约等条件太过于宽泛，因此可以采取${|V-WH|}_{2}^2$小于某一值或者${|W_{i+1}H_{i+1}-W_iH_i|_2^2}$小于某一个误差值。

以下是自己实现的代码：

```python
import numpy as np
import pandas as pd
import os

path = ".\\16_28workDay\\20111116.csv"
col_name = range(0,99)
data = pd.read_csv(path,names = list(col_name))
data.drop(axis = 1,columns = [0,1,2],inplace = True)


def NMF(V,r,error):
    n,p = V.shape
    W = np.random.rand(n,r)
    H = np.random.rand(r,p)
    H1 = H*(np.dot(W.T,V)/((W.T).dot(W).dot(H))+1e-6)
    W1 = W*(np.dot(V,H1.T)/(W.dot(H1).dot(H1.T))+1e-6)
    value = V.values
    count = 0
    while (np.linalg.norm(value-W.dot(H),ord = 'fro')-np.linalg.norm(value-W1.dot(H1),ord = 'fro'))>error:
        count+=1
        H = H1
        W = W1
        H1 = H*(np.dot(W.T,V)/((W.T).dot(W).dot(H)+ 1e-6))
        W1 = W*(np.dot(V,H1.T)/(W.dot(H1).dot(H1.T)+ 1e-6))
    return W1,H1
 
(W,H) = NMF(data,3,1e-4)
```

### 3.2.2 库函数实现

需要用pip下载Nimfa库，利用Nimfa库类来实现分解，代码如下：

```python
import pandas as pd
import nimfa

path = ".\\16_28workDay\\20111116.csv"
col_name = range(0,99)
data = pd.read_csv(path,names = list(col_name))
data.drop(axis = 1,columns = [0,1,2],inplace = True)

V = data.values

nmf = nimfa.Nmf(V, max_iter=200, rank=3, update='euclidean', objective='fro')
nmf_fit = nmf()

W16 = nmf_fit.basis()
H16 = nmf_fit.coef()

print(W16.shape)
print(H16.shape) #基矩阵
```

在本项目中，由于无法满足计算效率，因此在项目中还是采用调用Nimfa库来实现非负矩阵分解。在研究Nimfa源代码时发现，由于在代码中运用Numpy库的某些特性和特殊的函数调用，使内存占用较小例如numpy.asmatrix、函数回调。还有为提高代码运行速度，采用类和设置相对简单的结束循环条件。

## 3.3 分解结果

本来应该将9个流量矩阵合并成大矩阵127049x564，但是由于计算机算力没有达到相应要求，因此将9个流量矩阵一一分解，得到的基矩阵。为了便于聚合是画图，因此选用$r=3$时分解。得到的基矩阵可视化表现如下（相同初始值）：

![](4.png)

由上面的图形可以看出分解出来的基矩阵的变化基本相同，由此，将9个矩阵合并计算平均值，之后再对合并的矩阵进行分解，得到的图像如下图所示：

![](5.png)

# 四、聚合

本次项目需要建立检测道路突发异常的模型，而在突发异常检测中，我们一方面关注的是某条道路和原来该时刻的比较，比如，原来这条路上该时刻15分钟内的车流量为20辆，而需要预测的该时刻流量变成了80辆，此时这条路有非常大的可能性发生了交通道路异常。另一方面，我们关注和邻居道路的比较，也就是同等级的道路，比如，一条道路的车流量变化和另一条道路相同，但是在需要预测的时刻，同等级道路在该15分钟内车流量为20辆，但是该条道路在这15分钟内车流量为80辆，因此这条道路有很大的可能性出现了交通道路异常。

目前在大数据领域用得比较多的聚合方法有K-Means，K中心和DBSCAN。根据上述分解之后得到的系数矩阵，画出三维图像为：

![](2.png)

由该图像可知，大多数点都是聚合在一起的，没有非常明确的分界线或者是密度差，因此用DBSCAN不会获得很好的效果。我们又考虑到训练效率，因此我们选用K-Means算法实现聚类。

## 4.1 聚合方法

**邻居道路的定义：**我们将邻居道路定义为一群道路在相同时刻车流量相似。由于对大的流量矩阵进行了NMF操作，将大的流量矩阵降维到了系数矩阵。所以对邻居道路的聚类，可以直接针对系数矩阵进行聚类就可以完成。

**K-Means主要步骤：**

1. 随机选取K个点作为中心，K为自定义整数
2. 根据这k个中心，计算每一个点距离这K个中心的距离，某点离哪一个最近即为和这个点一类
3. 根据所分得的类计算该聚簇的重心，即这一个的横坐标的平均值和纵坐标的平均值所表示的点，一共得到新的中心点K个
4. 返回步骤2，直到中心点的位置不变，或者每一类中点不发生改变，即聚类完成。

## 4.2 具体实现

以下是自己实现的K中心的代码：

```python
def getKRamdonInt(K,n):
    i = []
    while len(i)!=K:
        m = random.randint(0,n)
        if i.count(m) == 0:
            i.append(m)
    return i

def KMeans(M,k):
    m = M.shape[0]
    init = np.array(getKRamdonInt(k,m-1))
    center = np.zeros((k,1))
    tmp = init 
    while np.linalg.norm(tmp-center) > 0.1: #误差范围
        center = tmp
        result = np.zeros((m,1))
        for i in range(0,m):
            dis = np.zeros((k,1))
            for j in range(0,k):
                dis[j,:] = np.linalg.norm(M[i,:]-M[center[j],:])
            result[i] = np.where(dis == dis.min(axis = 0))[0][0]
            
        result = pd.DataFrame(result,columns = ["label"])
        result = result.sort_values(by = "label").reset_index(drop = False) #将index信息留下
        result = result.rename(columns = {"index":"num"})
    
        # 选择中心点
        for j in range(0,k):
            label = result[result["label"]==j].reset_index(drop = True) #需要对label的index重新排序，会对插入列造成影响
            disSum = [[sum([np.linalg.norm(M[j]-M[i]) for i in label["num"]])] for j in label["num"] ]
            label = pd.concat([label,pd.DataFrame(disSum,columns = ["disSum"])],axis = 1)
            tmp[j] = label["num"][label["disSum"].idxmin(axis=0)]
            
    return center,result
```

同样由于运行效率问题，还是选用库函数Scikit-learn中的KMeans聚类函数，使用方法如下：

```python
from sklearn.cluster import KMeans
cluster = KMeans(init='k-means++', n_clusters=clusterNumber, n_init=10).fit(H.T)
label = cluster.labels_
```

## 4.3 聚合结果

我们设置一共有200类，即clusterNumber=200，以下用图形表示聚类之后的结果

![](Figure_1.png)

有图可以看到，有大多数的点都集中在零附近。这个可能对模型的预测造成较大的影响

# 五、评分函数

由第四部分可知，对于异常的评分主要分为历史数据比较与邻居道路数据比较，我们分别称之为历史模型和邻居道路模型。历史模型中主要由已有的数据建立概率模型，而在邻居道路模型中，主要由需要预测的那一天的数据建立模型。由于在现实生活中，交通路段上发生异常会持续较长的一段时间。所以基于计算效率和现实的需要，我们将时间间隔调整为1个小时。

## 5.1 历史模型

**历史模型的主要思路：**将9天的数据为离散值，通过该时段的数据建立正态分布模型。在评分时，将需要预测的数据放入改模型中进行计算，计算所得到的概率。

主要步骤如下：

1. 取出某一路段某一时段的9天的数据：$x_1,x_2 \cdots x_9$

2. 用这9天的数据建立正态分布模型，计算出其平均值和标准差：
   $$
   \bar x = \frac{x_1+x_2+ \cdots+x_9}{9}
   $$

   $$
   \sigma(x) = \sqrt{\frac{\sum_{i=1}^9{(x_i-\bar x)}^2}{8}}
   $$

   

3. 利用得到平均值和标准差，计算某段路某时刻车流量出现的概率

$$
p_h(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{{(x-\bar x)}^2}{2\sigma^2}}
$$



需要对矩阵中的每一个路段每一个时间段进行计算。当然在计算历史模型的时候可以加入邻居道路的9天数据，可以让预测结果更加准确。

**注意：**在计算历史模型的时候，可能存在计算出来的$\bar x$和$\sigma(x)$为0或者接近于0。在这种情况下，用正态分布计算出来的概率一定处于异常范围。但是分析如下：如果$\bar x$为零，则表示该条路段一直没有车通过，所以一定是正常道路；还有一种情况是$\sigma(x)$非常小，导致如果有非常微小的偏差就会让其处于异常值。为了避免这种情况，所以重新设置$\sigma(x)$的值。

## 5.2 邻居道路模型

**邻居道路模型的主要思路：**直接运用需要预测的数据，通过建立该时段的下邻居道路的正态分布模型。在评分时直接计算在某时刻的某个聚类中的概率。

主要步骤如下：

1. 由第四部分可知，已经将所有的道路分成200类，选取某一类，设某一类中道路的个数为$N$，即从$X_1,X_2 \cdots X_N$

2. 计算每一类中的平均值和标准差：
   $$
   \bar X = \frac{X_1+X_2+\cdots+X_N}{N}
   $$

   $$
   \sigma(X) = \sqrt{\frac{\sum_{i=1}^N{(X_i-\bar X)}^2}{N-1}}
   $$

   

3. 利用每一类中计算的标准差和平均值建立正态分布模型，并根据模型计算每一类中，不同路段相同时刻，车流量出现的概率
   $$
   p_a(X)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{{(X-\bar X)}^2}{2\sigma^2}}
   $$

**注意：**在计算邻居模型的时候，可能存在计算出来的$\bar  X$和$\sigma(X)$为0或者接近于0。在这种情况下，用正态分布计算出来的概率一定处于异常范围。但是分析如下：如果$\bar X$为零，则表示该条路段一直没有车通过，所以一定是正常道路；还有一种情况是$\sigma(X)$非常小，导致如果有非常微小的偏差就会让其处于异常值。而$\sigma(X)$较小，说明这几个邻居道路的车流量十分相近，因此也是正常值范围。

## 5.3 组合

由上述两个步骤得到了两个概率，需要对这两个概率进行组合，我们采用的方法是将这两个概率按照系数相加，最后得到的概率为：
$$
f(x) =\beta \times p_h(x)+(1-\beta) \times p_a(x)
$$
其中$\beta$的值按照肘方法得出，当交叉验证$f$值最大时，$\beta$就取该值。

**对$\beta$的取值进行估计：**可以分别对历史模型和邻居模型进行计算，预先估计查准率和查全率。

1. 对历史模型进行估计

   同样阈值设置到$10^{-50}$还有224个异常点，但是出现时间较为准确，如下图所示：

   <img src="historyResult.jpg" style="zoom:67%;" />

   

2. 对邻居模型进行估计

   在对邻居模型估计的时候，阈值设置到$10^{-50}$还有595个异常点，且这些异常点大多数都是在凌晨出现。如下图所示：

   ![](result.jpg)

## 六、模型评价

在第二部分数据清理的时候，引入了开源的OpenStreetMap库。同样在模型评价阶段，需要引入该开源的地图库，判断预测的事故点是否在真实事故点的附近。

## 6.1 评价方法

由第五部分计算出来，某种流量在历史判断和邻居判断下出现的概率，并对概率设置阈值，如果概率小于这一阈值，则可以预测该路段出现了异常。即
$$
f(x) \leq threshold
$$
则此时$x$对应的路段和时间点出现异常。

**附近的判断：**从网上爬取到当天的真实异常情况，以真实的异常画出一个半径为1Km的圆，如果我们预测到的路段在该子图内，则可以判断预测正确。

**交叉验证：**交叉验证的表格如下：

| 真实情况 |      | 预测结果 |      |
| -------- | ---- | -------- | ---- |
|          |      | 正例     | 反例 |
| 正例     |      | TP       | TN   |
| 反例     |      | FP       | FN   |

其中TP表示预测存在异常且真实异常的个数，也可以表示出真实存在异常且预测出来的个数。TN表示真实存在异常，但是没有预测出来的数据。FP表示预测出来异常但是真实没有异常的个数，这个的个数可能存在偏差，因为有些可能是由于本事信息的不完善。FN表示没有预测出来的点且也没有存在异常的个数，这个数据在项目中无法计算。

## 6.2 评价结果

### 6.2.1 评价标准

在做项目的时候，考虑到真实的事故发生之后，会造成较长路段和时间的拥堵。因此会存在真实的事故发生点和通过预测得到的路段存则一定距离，为此我们提出来三种评价标准的方案：

1. 把同一时间的段的所有点聚合在一起，然后计算事故发生点在道路上的最近点是否在这些点所组成的边上。但是考虑到可能会存在一个时间段发生多起事故。这样预测结果会出现在同一时间段不同地点都有点的存在。如果在这种情况下，判断事故点是否在这些点组成的边上，会造成计算真正例的不准确。因此这种方案被取消。

2. 在同一时间段，一段一段的路线进行比较，选取该段道路的两端，再取其一层邻近点，查看真实的事故发生点在道路上的最近点是否为这些点的其中之一。想法虽好，但是在实际操作中，预测出来的点大多数为十字路口上的点，因此这些点的邻近点较近，如下图所示：

   <img src="erroMap.jpg" style="zoom:80%;" />

   从图中可以看出，虽然事故发生点就在预测点的附近，但是由于邻近点的层数无法得知，导致	预测真实的点没有判断出来。而且如果出现高架桥等情况，在桥上的异常点通过在道路上的最	近点，肯能会被放到高架桥下，也会导致真实预测点没有被判断出来。

3. 同一时间段，计算距离，如果距离小于某值，则预测到了真实的事故点。我们项目是通过这种方法计算查全率和查准率。但是距离阈值较为主观，无法客观的反应预测准确的点。

### 6.2.2 预测结果

**查全率：**53.8%

**查准率：**37.5%

**f：**0.4

**预测结果图示：**

<img src="rightMap.jpg" style="zoom:80%;" />

# 七、总结

## 7.1 改进思路

**1. 收集数据：**可采用监控来判断车流量，不用进行道路匹配，从而减小产生的误差。

**2. 车流量处理：**由于数据较少，可以对车流量进行离散化处理，以提高历史评分的精确度。

在真实处理中，对流量矩阵进行离散化后，容易出现历史数据的标准差都为0，从而降低预测的准确率。因此需要更为巧妙的离散化方法。或许根据每类邻居道路的某一个时间段进行离散化能够提高预测的准确度。

**3. 正确用例的判断：**可以通过判断离事故点最近的道路点是否在某一条道路上

在上面所讨论的评价标准中，主观因素占据极大部分，造成了某些利用肘方法的得到的变量可能不太准确，最终导致预测模型的迁移性较差。如果能设置一个较为公正的评价方式，则会对变量的设置产生极大的影响。比如说如果真实异常点与预测出来的异常点在同一个街区，则能说明预测成功。

## 7.2 心得体会

在这次项目中，我自己动手写了关于大数据处理的算法，更加熟悉了python以及其扩展库。也学会了更高效的查找有用的信息。同时我也更熟悉了关于Anaconda的一些虚拟环境配置，真正体会到到python发生环境冲突的易发生性。

在项目中，我认识到了团队合作的重要性和时间管理的重要性。在团队合作方面，要在项目开始前就了解到队友的长处和不足，针对个人能力进行分工，让每一个人在自己更加熟悉的领域工作，能更快的推进项目的进度。在时间管理上面，虽然无法预测每一个模块什么时候完成，但是一定要有最终期限和一定的提前量，这样在最终的项目交付上就不会太匆忙。

最后，感谢老师对我们项目的指导和团队成员们的支持和鼓励！





[^1]: 高德地图《2018年度中国主要城市交通分析报告》
[^2]: Yuan J ,  Zheng Y ,  Zhang C , et al. An Interactive-Voting Based Map Matching Algorithm[C]// Eleventh International Conference on Mobile Data Management, MDM 2010, Kanas City, Missouri, USA, 23-26 May 2010. IEEE, 2010.
[^3]:WANGKe-jun, ZUOChun-ting. 非负矩阵分解特征提取技术的研究进展[J]. 计算机应用研究, 2014, 31(4):970-975.

